# Test configuration for separator models
# This config is used with test.py for evaluating trained separator models

# Wandb configuration
wandb_entity: cp_tobi

# Data copying configuration
enable_data_copy: true

deterministic: true
manual_seed: 0

datamodule:
    module: src.datamodules.datamodule
    main: DataModule
    args:
        test_dataloader:
            batch_size: 2
            num_workers: 16
            persistent_workers: false
            dataset:
                module: src.datamodules.dataset.s5.dataset_s5_waveform
                main: DatasetS5Waveform
                args:
                    soundscape_dir: "data/dev_set/test/soundscape"
                    oracle_target_dir: "data/dev_set/test/oracle_target"
                    n_sources: 3
                    label_set: dcase2025t4
                    label_vector_mode: concat
                    sr: 32000

lightning_module:
    module: src.training.lightningmodule.labelqueried_1lb_separation
    main: LabelQueriedSeparationLightning1LB
    args:
        model:
            module: src.models.audiosep_resunet.resunet
            main: ResUNet30
            args:
                input_channels: 4
                output_channels: 1
                condition_size: 512
                target_sources_num: 1
                label_len: 18
                use_time_film: true
                time_film_mode: additive
                channels_last: true
                load_pretrained: true
                hop_size: 160
                window_size: 2048
                binarize_sed_preds: false
                max_iterations: 0
                detach_feedback: true
                dprnn:
                    use: false
                    post_sb: true
                    type: GRU
                    layers: 2
                    hidden: 256
                    dropout: 0.1
        soundbeam:
            apply: true
            merge_method: add
            trainable: true
            lr: 0.0004
            lr_decay_factor: 0.9
            tagger_model:
                module: src.models.wrapper
                main: Wrapper
                args:
                    model_name: m2d
                    checkpoint: slok9mxd
                    head_type: attention
                    use_head_norm: false
                    num_classes: 18
                    ref_channel: 0
                    data_sr: 32000
                    target_sr: 16000
                    down_up_sample: true
        evaluation_tagger_model:
            module: src.models.m2dat.m2dat
            main: M2dAt
            args:
                weight_file: checkpoint/m2d_as_vit_base-80x1001p16x16p32k-240413_AS-FT_enconly/weights_ep69it3124-0.47998.pth
                checkpoint: baseline
                num_classes: 18
                ref_channel: 0
        loss:
            module: src.training.loss.snr
            main: get_loss_func
        optimizer:
            module: torch.optim
            main: AdamW
            split_params: true
            lr_decay_factor: 0.2
            lr_dprnn: null
            args:
                params: null
                lr: 0.001
                betas: [0.9, 0.999]
                eps: 0.00000001
                weight_decay: 0.0
                amsgrad: true
        lr_scheduler:
            schedule_mode: cos
            num_warmup_steps: 12000
            lr_end: 2e-7
        is_validation: true
        eval_checkpoint: baseline

train:
    trainer:
        module: lightning.pytorch
        main: Trainer
        compile: false
        torch_flags: false
        channels_last: false
        args:
            accelerator: auto
            devices: auto
            strategy: auto
            num_nodes: 1
            precision: 32-true
            logger: null
            fast_dev_run: False
            max_epochs: 1
            log_every_n_steps: 50
            use_distributed_sampler: True
            sync_batchnorm: False
            check_val_every_n_epoch: 1
            val_check_interval: 1.0
            num_sanity_val_steps: 0
            enable_checkpointing: False
            enable_progress_bar: True
            enable_model_summary: False
            limit_train_batches: null
            limit_val_batches: null
            gradient_clip_val: 0.5
            accumulate_grad_batches: 1
